{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Data Science\n",
    "## S1 Week 02: Pandas - Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning outcomes:** \n",
    "In this lab you will learn to prepare your data for future use. By the end of the lab you should be able to:\n",
    "- get an overview of your data,\n",
    "- clean data, and\n",
    "- combine data from multiple datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analysing the survival rate of different passenger groups on the titanic last week, now imagine you are a political advisor for the UN this week. You will analyse data from the World Health Organization (WHO), in order to answer the following question:\n",
    "\n",
    "**Research question:** Which medical, social and economic factors have the biggest impact on life expectancy, and which improvement would have the biggest impact on the three countries with the shortest life expectancy?\n",
    "\n",
    "**Data description:** The data from the first [dataset](https://www.kaggle.com/kumarajarshi/life-expectancy-who) for this lab originates from the Global Health Observatory (GHO) data repository under the WHO. It contains health factors, as well as social and economic data of 193 countries between 2000 and 2015. The dataset was slightly preprocessed for this lab. The [second dataset](https://data.world/fivethirtyeight/alcohol-consumption) is based on data from an article in 'Fivethirtyeight', a website that focuses on political and economic articles and polls. It contains information about the alcohol consumption in 193 countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks**\n",
    "- Starting from this lab, we will not provide all the code, but expect you to be able to draw from previous labs to fill in the code.\n",
    "- Try not to use copy+paste when coding these labs, as typing will help you memorize the code better.\n",
    "- Try to understand each detail in the code we provide, and read the comments!\n",
    "- In the text `df` refers to a generic pandas DataFrame, which allows us to indicate which methods are applied to DataFrames. E.g. in `df.head()`, the  `head` is a member function of a pandas DataFrame.\n",
    "- Finally, try to keep a **clean structure** when programming: As discussed in the previous lab, we will start by loading all libraries we need at the top of the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start by loading the data we will use in this lab.\n",
    "\n",
    "**Exercise 01:** \n",
    "\n",
    "a) Load the `life_expectancy.csv` file which is in `datasets`, and store the variable as `life_expectancy`. \n",
    "\n",
    "b) Print the first few entries of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Status</th>\n",
       "      <th>Life expectancy</th>\n",
       "      <th>Adult Mortality</th>\n",
       "      <th>infant deaths</th>\n",
       "      <th>percentage expenditure</th>\n",
       "      <th>Hepatitis B</th>\n",
       "      <th>Measles</th>\n",
       "      <th>BMI</th>\n",
       "      <th>...</th>\n",
       "      <th>Polio</th>\n",
       "      <th>Total expenditure</th>\n",
       "      <th>Diphtheria</th>\n",
       "      <th>HIV/AIDS</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Population</th>\n",
       "      <th>thinness 1-19 years</th>\n",
       "      <th>thinness 5-9 years</th>\n",
       "      <th>Income composition of resources</th>\n",
       "      <th>Schooling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2015</td>\n",
       "      <td>Developing</td>\n",
       "      <td>65.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>62</td>\n",
       "      <td>71.279624</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1154</td>\n",
       "      <td>19.1</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.16</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>584.259210</td>\n",
       "      <td>33736494.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>17.3</td>\n",
       "      <td>0.479</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2014</td>\n",
       "      <td>Developing</td>\n",
       "      <td>59.9</td>\n",
       "      <td>271.0</td>\n",
       "      <td>64</td>\n",
       "      <td>73.523582</td>\n",
       "      <td>62.0</td>\n",
       "      <td>492</td>\n",
       "      <td>18.6</td>\n",
       "      <td>...</td>\n",
       "      <td>58.0</td>\n",
       "      <td>8.18</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>612.696514</td>\n",
       "      <td>327582.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.476</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2013</td>\n",
       "      <td>Developing</td>\n",
       "      <td>59.9</td>\n",
       "      <td>268.0</td>\n",
       "      <td>66</td>\n",
       "      <td>73.219243</td>\n",
       "      <td>64.0</td>\n",
       "      <td>430</td>\n",
       "      <td>18.1</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>8.13</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>631.744976</td>\n",
       "      <td>31731688.0</td>\n",
       "      <td>17.7</td>\n",
       "      <td>17.7</td>\n",
       "      <td>0.470</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2012</td>\n",
       "      <td>Developing</td>\n",
       "      <td>59.5</td>\n",
       "      <td>272.0</td>\n",
       "      <td>69</td>\n",
       "      <td>78.184215</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2787</td>\n",
       "      <td>17.6</td>\n",
       "      <td>...</td>\n",
       "      <td>67.0</td>\n",
       "      <td>8.52</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>669.959000</td>\n",
       "      <td>3696958.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.463</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2011</td>\n",
       "      <td>Developing</td>\n",
       "      <td>59.2</td>\n",
       "      <td>275.0</td>\n",
       "      <td>71</td>\n",
       "      <td>7.097109</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3013</td>\n",
       "      <td>17.2</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>7.87</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>63.537231</td>\n",
       "      <td>2978599.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>18.2</td>\n",
       "      <td>0.454</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country  Year      Status  Life expectancy  Adult Mortality  \\\n",
       "0  Afghanistan  2015  Developing             65.0            263.0   \n",
       "1  Afghanistan  2014  Developing             59.9            271.0   \n",
       "2  Afghanistan  2013  Developing             59.9            268.0   \n",
       "3  Afghanistan  2012  Developing             59.5            272.0   \n",
       "4  Afghanistan  2011  Developing             59.2            275.0   \n",
       "\n",
       "   infant deaths  percentage expenditure  Hepatitis B  Measles   BMI  ...  \\\n",
       "0             62               71.279624         65.0     1154  19.1  ...   \n",
       "1             64               73.523582         62.0      492  18.6  ...   \n",
       "2             66               73.219243         64.0      430  18.1  ...   \n",
       "3             69               78.184215         67.0     2787  17.6  ...   \n",
       "4             71                7.097109         68.0     3013  17.2  ...   \n",
       "\n",
       "   Polio  Total expenditure Diphtheria  HIV/AIDS         GDP  Population  \\\n",
       "0    6.0               8.16       65.0       0.1  584.259210  33736494.0   \n",
       "1   58.0               8.18       62.0       0.1  612.696514    327582.0   \n",
       "2   62.0               8.13       64.0       0.1  631.744976  31731688.0   \n",
       "3   67.0               8.52       67.0       0.1  669.959000   3696958.0   \n",
       "4   68.0               7.87       68.0       0.1   63.537231   2978599.0   \n",
       "\n",
       "   thinness 1-19 years  thinness 5-9 years  Income composition of resources  \\\n",
       "0                 17.2                17.3                            0.479   \n",
       "1                 17.5                17.5                            0.476   \n",
       "2                 17.7                17.7                            0.470   \n",
       "3                 17.9                18.0                            0.463   \n",
       "4                 18.2                18.2                            0.454   \n",
       "\n",
       "   Schooling  \n",
       "0       10.1  \n",
       "1       10.0  \n",
       "2        9.9  \n",
       "3        9.8  \n",
       "4        9.5  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "life_expectancy = pd.read_csv('datasets/life_expectancy.csv')\n",
    "life_expectancy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lab you learned to load data, and get an overview of the data using `df.head()`. Now, we will look at higher level information of the data. Whenever you work with data, it is important to get a good idea of how your data looks like:\n",
    "- **Nature of data:** Is the data from a population (e.g. medical data from patients) or rather high-level statistics (e.g. economic data comparison of different countries)?\n",
    "- **Data format:** Is the dataset a relational database, if so what columns does it have?\n",
    "- **Missing data:** Is the data complete (e.g. does every row contain an entry in every column, or are some entries missing)?\n",
    "- **Consistent data:** Is the notation used consistent throughout the database (e.g. Nan vs None vs Null vs __ vs 0 vs -1 ...)?\n",
    "\n",
    "All those points and many more should be answered before you start running any algorithms on your data. **Clean data is key!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.A Data exploration: High level information of DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exercise 01 you printed out the first few entries and also the column names. These entries might give you an idea of what type of entries you should expect in a specific columns. For example, in the column 'Total expenditure', which reflects the total general government expenditure on health, you should only expect numbers. However, this might not always be the case and the data might be stored in a different format from what you would expect. Columns with numbers could be floats, integers or even strings, e.g. 0 vs 0.0 vs 'Zero' vs 'NIL' vs '0'.\n",
    "\n",
    "**Exercise 02:** Use filtering to get all the entries for 'Zimbabwe'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Year</th>\n",
       "      <th>Status</th>\n",
       "      <th>Life expectancy</th>\n",
       "      <th>Adult Mortality</th>\n",
       "      <th>infant deaths</th>\n",
       "      <th>percentage expenditure</th>\n",
       "      <th>Hepatitis B</th>\n",
       "      <th>Measles</th>\n",
       "      <th>BMI</th>\n",
       "      <th>...</th>\n",
       "      <th>Polio</th>\n",
       "      <th>Total expenditure</th>\n",
       "      <th>Diphtheria</th>\n",
       "      <th>HIV/AIDS</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Population</th>\n",
       "      <th>thinness 1-19 years</th>\n",
       "      <th>thinness 5-9 years</th>\n",
       "      <th>Income composition of resources</th>\n",
       "      <th>Schooling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2015</td>\n",
       "      <td>Developing</td>\n",
       "      <td>67.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.8</td>\n",
       "      <td>...</td>\n",
       "      <td>88.0</td>\n",
       "      <td>No data</td>\n",
       "      <td>87.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>118.693830</td>\n",
       "      <td>15777451.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.507</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2014</td>\n",
       "      <td>Developing</td>\n",
       "      <td>59.2</td>\n",
       "      <td>371.0</td>\n",
       "      <td>23</td>\n",
       "      <td>10.822595</td>\n",
       "      <td>91.0</td>\n",
       "      <td>0</td>\n",
       "      <td>31.3</td>\n",
       "      <td>...</td>\n",
       "      <td>92.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>91.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>127.474620</td>\n",
       "      <td>15411675.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.498</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2013</td>\n",
       "      <td>Developing</td>\n",
       "      <td>58.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>25</td>\n",
       "      <td>10.666707</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>6.88</td>\n",
       "      <td>95.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>111.227396</td>\n",
       "      <td>155456.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.488</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2012</td>\n",
       "      <td>Developing</td>\n",
       "      <td>56.6</td>\n",
       "      <td>429.0</td>\n",
       "      <td>26</td>\n",
       "      <td>92.602336</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>...</td>\n",
       "      <td>95.0</td>\n",
       "      <td>6.69</td>\n",
       "      <td>95.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>955.648466</td>\n",
       "      <td>1471826.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.464</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2011</td>\n",
       "      <td>Developing</td>\n",
       "      <td>54.9</td>\n",
       "      <td>464.0</td>\n",
       "      <td>28</td>\n",
       "      <td>63.750530</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>...</td>\n",
       "      <td>93.0</td>\n",
       "      <td>6.31</td>\n",
       "      <td>93.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>839.927936</td>\n",
       "      <td>14386649.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.452</td>\n",
       "      <td>10.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2010</td>\n",
       "      <td>Developing</td>\n",
       "      <td>52.4</td>\n",
       "      <td>527.0</td>\n",
       "      <td>29</td>\n",
       "      <td>53.308581</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9696</td>\n",
       "      <td>29.4</td>\n",
       "      <td>...</td>\n",
       "      <td>89.0</td>\n",
       "      <td>5.37</td>\n",
       "      <td>89.0</td>\n",
       "      <td>15.7</td>\n",
       "      <td>713.635620</td>\n",
       "      <td>1486317.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.436</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2009</td>\n",
       "      <td>Developing</td>\n",
       "      <td>50.0</td>\n",
       "      <td>587.0</td>\n",
       "      <td>30</td>\n",
       "      <td>1.040021</td>\n",
       "      <td>73.0</td>\n",
       "      <td>853</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>6.26</td>\n",
       "      <td>73.0</td>\n",
       "      <td>18.1</td>\n",
       "      <td>65.824121</td>\n",
       "      <td>1381599.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.419</td>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2008</td>\n",
       "      <td>Developing</td>\n",
       "      <td>48.2</td>\n",
       "      <td>632.0</td>\n",
       "      <td>30</td>\n",
       "      <td>20.843429</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.6</td>\n",
       "      <td>...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>4.96</td>\n",
       "      <td>75.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>325.678573</td>\n",
       "      <td>13558469.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.421</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2007</td>\n",
       "      <td>Developing</td>\n",
       "      <td>46.6</td>\n",
       "      <td>67.0</td>\n",
       "      <td>29</td>\n",
       "      <td>29.814566</td>\n",
       "      <td>72.0</td>\n",
       "      <td>242</td>\n",
       "      <td>28.2</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>4.47</td>\n",
       "      <td>73.0</td>\n",
       "      <td>23.7</td>\n",
       "      <td>396.998217</td>\n",
       "      <td>1332999.0</td>\n",
       "      <td>8.2</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.414</td>\n",
       "      <td>9.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2006</td>\n",
       "      <td>Developing</td>\n",
       "      <td>45.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>28</td>\n",
       "      <td>34.262169</td>\n",
       "      <td>68.0</td>\n",
       "      <td>212</td>\n",
       "      <td>27.9</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>5.12</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.8</td>\n",
       "      <td>414.796232</td>\n",
       "      <td>13124267.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.408</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2932</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2005</td>\n",
       "      <td>Developing</td>\n",
       "      <td>44.6</td>\n",
       "      <td>717.0</td>\n",
       "      <td>28</td>\n",
       "      <td>8.717409</td>\n",
       "      <td>65.0</td>\n",
       "      <td>420</td>\n",
       "      <td>27.5</td>\n",
       "      <td>...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>6.44</td>\n",
       "      <td>68.0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>444.765750</td>\n",
       "      <td>129432.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.406</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2004</td>\n",
       "      <td>Developing</td>\n",
       "      <td>44.3</td>\n",
       "      <td>723.0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.0</td>\n",
       "      <td>31</td>\n",
       "      <td>27.1</td>\n",
       "      <td>...</td>\n",
       "      <td>67.0</td>\n",
       "      <td>7.13</td>\n",
       "      <td>65.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>454.366654</td>\n",
       "      <td>12777511.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.407</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2003</td>\n",
       "      <td>Developing</td>\n",
       "      <td>44.5</td>\n",
       "      <td>715.0</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>998</td>\n",
       "      <td>26.7</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.52</td>\n",
       "      <td>68.0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>453.351155</td>\n",
       "      <td>12633897.0</td>\n",
       "      <td>9.8</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.418</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2002</td>\n",
       "      <td>Developing</td>\n",
       "      <td>44.8</td>\n",
       "      <td>73.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>304</td>\n",
       "      <td>26.3</td>\n",
       "      <td>...</td>\n",
       "      <td>73.0</td>\n",
       "      <td>6.53</td>\n",
       "      <td>71.0</td>\n",
       "      <td>39.8</td>\n",
       "      <td>57.348340</td>\n",
       "      <td>125525.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.427</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2001</td>\n",
       "      <td>Developing</td>\n",
       "      <td>45.3</td>\n",
       "      <td>686.0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>76.0</td>\n",
       "      <td>529</td>\n",
       "      <td>25.9</td>\n",
       "      <td>...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>6.16</td>\n",
       "      <td>75.0</td>\n",
       "      <td>42.1</td>\n",
       "      <td>548.587312</td>\n",
       "      <td>12366165.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.427</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2000</td>\n",
       "      <td>Developing</td>\n",
       "      <td>46.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1483</td>\n",
       "      <td>25.5</td>\n",
       "      <td>...</td>\n",
       "      <td>78.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>78.0</td>\n",
       "      <td>43.5</td>\n",
       "      <td>547.358879</td>\n",
       "      <td>12222251.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.434</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country  Year      Status  Life expectancy  Adult Mortality  \\\n",
       "2922  Zimbabwe  2015  Developing             67.0            336.0   \n",
       "2923  Zimbabwe  2014  Developing             59.2            371.0   \n",
       "2924  Zimbabwe  2013  Developing             58.0            399.0   \n",
       "2925  Zimbabwe  2012  Developing             56.6            429.0   \n",
       "2926  Zimbabwe  2011  Developing             54.9            464.0   \n",
       "2927  Zimbabwe  2010  Developing             52.4            527.0   \n",
       "2928  Zimbabwe  2009  Developing             50.0            587.0   \n",
       "2929  Zimbabwe  2008  Developing             48.2            632.0   \n",
       "2930  Zimbabwe  2007  Developing             46.6             67.0   \n",
       "2931  Zimbabwe  2006  Developing             45.4              7.0   \n",
       "2932  Zimbabwe  2005  Developing             44.6            717.0   \n",
       "2933  Zimbabwe  2004  Developing             44.3            723.0   \n",
       "2934  Zimbabwe  2003  Developing             44.5            715.0   \n",
       "2935  Zimbabwe  2002  Developing             44.8             73.0   \n",
       "2936  Zimbabwe  2001  Developing             45.3            686.0   \n",
       "2937  Zimbabwe  2000  Developing             46.0            665.0   \n",
       "\n",
       "      infant deaths  percentage expenditure  Hepatitis B  Measles   BMI  ...  \\\n",
       "2922             22                0.000000         87.0        0  31.8  ...   \n",
       "2923             23               10.822595         91.0        0  31.3  ...   \n",
       "2924             25               10.666707         95.0        0   3.8  ...   \n",
       "2925             26               92.602336         97.0        0   3.3  ...   \n",
       "2926             28               63.750530         94.0        0  29.9  ...   \n",
       "2927             29               53.308581          9.0     9696  29.4  ...   \n",
       "2928             30                1.040021         73.0      853  29.0  ...   \n",
       "2929             30               20.843429         75.0        0  28.6  ...   \n",
       "2930             29               29.814566         72.0      242  28.2  ...   \n",
       "2931             28               34.262169         68.0      212  27.9  ...   \n",
       "2932             28                8.717409         65.0      420  27.5  ...   \n",
       "2933             27                0.000000         68.0       31  27.1  ...   \n",
       "2934             26                0.000000          7.0      998  26.7  ...   \n",
       "2935             25                0.000000         73.0      304  26.3  ...   \n",
       "2936             25                0.000000         76.0      529  25.9  ...   \n",
       "2937             24                0.000000         79.0     1483  25.5  ...   \n",
       "\n",
       "      Polio  Total expenditure Diphtheria  HIV/AIDS         GDP  Population  \\\n",
       "2922   88.0            No data       87.0       6.2  118.693830  15777451.0   \n",
       "2923   92.0               6.44       91.0       6.3  127.474620  15411675.0   \n",
       "2924   95.0               6.88       95.0       6.8  111.227396    155456.0   \n",
       "2925   95.0               6.69       95.0       8.8  955.648466   1471826.0   \n",
       "2926   93.0               6.31       93.0      13.3  839.927936  14386649.0   \n",
       "2927   89.0               5.37       89.0      15.7  713.635620   1486317.0   \n",
       "2928   69.0               6.26       73.0      18.1   65.824121   1381599.0   \n",
       "2929   75.0               4.96       75.0      20.5  325.678573  13558469.0   \n",
       "2930   73.0               4.47       73.0      23.7  396.998217   1332999.0   \n",
       "2931   71.0               5.12        7.0      26.8  414.796232  13124267.0   \n",
       "2932   69.0               6.44       68.0      30.3  444.765750    129432.0   \n",
       "2933   67.0               7.13       65.0      33.6  454.366654  12777511.0   \n",
       "2934    7.0               6.52       68.0      36.7  453.351155  12633897.0   \n",
       "2935   73.0               6.53       71.0      39.8   57.348340    125525.0   \n",
       "2936   76.0               6.16       75.0      42.1  548.587312  12366165.0   \n",
       "2937   78.0                7.1       78.0      43.5  547.358879  12222251.0   \n",
       "\n",
       "      thinness 1-19 years  thinness 5-9 years  \\\n",
       "2922                  5.6                 5.5   \n",
       "2923                  5.9                 5.7   \n",
       "2924                  6.2                 6.0   \n",
       "2925                  6.5                 6.4   \n",
       "2926                  6.8                 6.7   \n",
       "2927                  7.1                 7.0   \n",
       "2928                  7.5                 7.4   \n",
       "2929                  7.8                 7.8   \n",
       "2930                  8.2                 8.2   \n",
       "2931                  8.6                 8.6   \n",
       "2932                  9.0                 9.0   \n",
       "2933                  9.4                 9.4   \n",
       "2934                  9.8                 9.9   \n",
       "2935                  1.2                 1.3   \n",
       "2936                  1.6                 1.7   \n",
       "2937                 11.0                11.2   \n",
       "\n",
       "      Income composition of resources  Schooling  \n",
       "2922                            0.507       10.3  \n",
       "2923                            0.498       10.3  \n",
       "2924                            0.488       10.4  \n",
       "2925                            0.464        9.8  \n",
       "2926                            0.452       10.1  \n",
       "2927                            0.436       10.0  \n",
       "2928                            0.419        9.9  \n",
       "2929                            0.421        9.7  \n",
       "2930                            0.414        9.6  \n",
       "2931                            0.408        9.5  \n",
       "2932                            0.406        9.3  \n",
       "2933                            0.407        9.2  \n",
       "2934                            0.418        9.5  \n",
       "2935                            0.427       10.0  \n",
       "2936                            0.427        9.8  \n",
       "2937                            0.434        9.8  \n",
       "\n",
       "[16 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "life_expectancy[life_expectancy.Country == 'Zimbabwe']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As you can see it contains a string 'No data' in the 'Total expenditure' column. If we print out the entry below, which looks like a float (6.44), we realize something unexpected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.44'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "life_expectancy.iloc[2923]['Total expenditure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's printed out as '6.44' instead of 6.44. This is because pandas expects all entries in a specific column to be of the same type. Thus, when pandas loads the table from a csv file and finds an entry that clearly isn't a numeric value ('No data'), it casts all other entries in the same column to a string.\n",
    "\n",
    "To be certain of **what types are used** we can use `df.info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2938 entries, 0 to 2937\n",
      "Data columns (total 21 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   Country                          2938 non-null   object \n",
      " 1   Year                             2938 non-null   int64  \n",
      " 2   Status                           2938 non-null   object \n",
      " 3   Life expectancy                  2928 non-null   float64\n",
      " 4   Adult Mortality                  2928 non-null   float64\n",
      " 5   infant deaths                    2938 non-null   int64  \n",
      " 6   percentage expenditure           2938 non-null   float64\n",
      " 7   Hepatitis B                      2385 non-null   float64\n",
      " 8   Measles                          2938 non-null   int64  \n",
      " 9   BMI                              2904 non-null   float64\n",
      " 10  under-five deaths                2938 non-null   int64  \n",
      " 11  Polio                            2919 non-null   float64\n",
      " 12  Total expenditure                2938 non-null   object \n",
      " 13  Diphtheria                       2919 non-null   float64\n",
      " 14  HIV/AIDS                         2938 non-null   float64\n",
      " 15  GDP                              2490 non-null   float64\n",
      " 16  Population                       2286 non-null   float64\n",
      " 17  thinness 1-19 years              2904 non-null   float64\n",
      " 18  thinness 5-9 years               2904 non-null   float64\n",
      " 19  Income composition of resources  2771 non-null   float64\n",
      " 20  Schooling                        2775 non-null   float64\n",
      "dtypes: float64(14), int64(4), object(3)\n",
      "memory usage: 482.1+ KB\n"
     ]
    }
   ],
   "source": [
    "life_expectancy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** The 'Total expenditure' column has `dtype:object`, but the entries we printed out above looked like strings. To understand why pandas stores strings as objects, read this answer on [stackoverflow](https://stackoverflow.com/questions/21018654/strings-in-a-dataframe-but-dtype-is-object/21020411). **Stackoverflow** is an extremely helpful website for most questions you will encounter when coding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an **overview of the numerical data** you have in your dataset `.describe()` is a good option. It gives you the most important statistical analysis of your columns with numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Life expectancy</th>\n",
       "      <th>Adult Mortality</th>\n",
       "      <th>infant deaths</th>\n",
       "      <th>percentage expenditure</th>\n",
       "      <th>Hepatitis B</th>\n",
       "      <th>Measles</th>\n",
       "      <th>BMI</th>\n",
       "      <th>under-five deaths</th>\n",
       "      <th>Polio</th>\n",
       "      <th>Diphtheria</th>\n",
       "      <th>HIV/AIDS</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Population</th>\n",
       "      <th>thinness 1-19 years</th>\n",
       "      <th>thinness 5-9 years</th>\n",
       "      <th>Income composition of resources</th>\n",
       "      <th>Schooling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2928.000000</td>\n",
       "      <td>2928.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2385.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2904.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2919.000000</td>\n",
       "      <td>2919.000000</td>\n",
       "      <td>2938.000000</td>\n",
       "      <td>2490.000000</td>\n",
       "      <td>2.286000e+03</td>\n",
       "      <td>2904.000000</td>\n",
       "      <td>2904.000000</td>\n",
       "      <td>2771.000000</td>\n",
       "      <td>2775.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2007.518720</td>\n",
       "      <td>69.224932</td>\n",
       "      <td>164.796448</td>\n",
       "      <td>30.303948</td>\n",
       "      <td>738.251295</td>\n",
       "      <td>80.940461</td>\n",
       "      <td>2419.592240</td>\n",
       "      <td>38.321247</td>\n",
       "      <td>42.035739</td>\n",
       "      <td>82.550188</td>\n",
       "      <td>82.324084</td>\n",
       "      <td>1.742103</td>\n",
       "      <td>7483.158469</td>\n",
       "      <td>1.275338e+07</td>\n",
       "      <td>4.839704</td>\n",
       "      <td>4.870317</td>\n",
       "      <td>0.627551</td>\n",
       "      <td>11.992793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.613841</td>\n",
       "      <td>9.523867</td>\n",
       "      <td>124.292079</td>\n",
       "      <td>117.926501</td>\n",
       "      <td>1987.914858</td>\n",
       "      <td>25.070016</td>\n",
       "      <td>11467.272489</td>\n",
       "      <td>20.044034</td>\n",
       "      <td>160.445548</td>\n",
       "      <td>23.428046</td>\n",
       "      <td>23.716912</td>\n",
       "      <td>5.077785</td>\n",
       "      <td>14270.169342</td>\n",
       "      <td>6.101210e+07</td>\n",
       "      <td>4.420195</td>\n",
       "      <td>4.508882</td>\n",
       "      <td>0.210904</td>\n",
       "      <td>3.358920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>36.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.681350</td>\n",
       "      <td>3.400000e+01</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2004.000000</td>\n",
       "      <td>63.100000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.685343</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>463.935626</td>\n",
       "      <td>1.957932e+05</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.493000</td>\n",
       "      <td>10.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2008.000000</td>\n",
       "      <td>72.100000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>64.912906</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>43.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1766.947595</td>\n",
       "      <td>1.386542e+06</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>0.677000</td>\n",
       "      <td>12.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2012.000000</td>\n",
       "      <td>75.700000</td>\n",
       "      <td>228.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>441.534144</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>360.250000</td>\n",
       "      <td>56.200000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>5910.806335</td>\n",
       "      <td>7.420359e+06</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>14.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>723.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>19479.911610</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>212183.000000</td>\n",
       "      <td>87.300000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>50.600000</td>\n",
       "      <td>119172.741800</td>\n",
       "      <td>1.293859e+09</td>\n",
       "      <td>27.700000</td>\n",
       "      <td>28.600000</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>20.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Year  Life expectancy  Adult Mortality  infant deaths  \\\n",
       "count  2938.000000      2928.000000      2928.000000    2938.000000   \n",
       "mean   2007.518720        69.224932       164.796448      30.303948   \n",
       "std       4.613841         9.523867       124.292079     117.926501   \n",
       "min    2000.000000        36.300000         1.000000       0.000000   \n",
       "25%    2004.000000        63.100000        74.000000       0.000000   \n",
       "50%    2008.000000        72.100000       144.000000       3.000000   \n",
       "75%    2012.000000        75.700000       228.000000      22.000000   \n",
       "max    2015.000000        89.000000       723.000000    1800.000000   \n",
       "\n",
       "       percentage expenditure  Hepatitis B        Measles          BMI  \\\n",
       "count             2938.000000  2385.000000    2938.000000  2904.000000   \n",
       "mean               738.251295    80.940461    2419.592240    38.321247   \n",
       "std               1987.914858    25.070016   11467.272489    20.044034   \n",
       "min                  0.000000     1.000000       0.000000     1.000000   \n",
       "25%                  4.685343    77.000000       0.000000    19.300000   \n",
       "50%                 64.912906    92.000000      17.000000    43.500000   \n",
       "75%                441.534144    97.000000     360.250000    56.200000   \n",
       "max              19479.911610    99.000000  212183.000000    87.300000   \n",
       "\n",
       "       under-five deaths        Polio   Diphtheria     HIV/AIDS  \\\n",
       "count        2938.000000  2919.000000  2919.000000  2938.000000   \n",
       "mean           42.035739    82.550188    82.324084     1.742103   \n",
       "std           160.445548    23.428046    23.716912     5.077785   \n",
       "min             0.000000     3.000000     2.000000     0.100000   \n",
       "25%             0.000000    78.000000    78.000000     0.100000   \n",
       "50%             4.000000    93.000000    93.000000     0.100000   \n",
       "75%            28.000000    97.000000    97.000000     0.800000   \n",
       "max          2500.000000    99.000000    99.000000    50.600000   \n",
       "\n",
       "                 GDP    Population  thinness 1-19 years  thinness 5-9 years  \\\n",
       "count    2490.000000  2.286000e+03          2904.000000         2904.000000   \n",
       "mean     7483.158469  1.275338e+07             4.839704            4.870317   \n",
       "std     14270.169342  6.101210e+07             4.420195            4.508882   \n",
       "min         1.681350  3.400000e+01             0.100000            0.100000   \n",
       "25%       463.935626  1.957932e+05             1.600000            1.500000   \n",
       "50%      1766.947595  1.386542e+06             3.300000            3.300000   \n",
       "75%      5910.806335  7.420359e+06             7.200000            7.200000   \n",
       "max    119172.741800  1.293859e+09            27.700000           28.600000   \n",
       "\n",
       "       Income composition of resources    Schooling  \n",
       "count                      2771.000000  2775.000000  \n",
       "mean                          0.627551    11.992793  \n",
       "std                           0.210904     3.358920  \n",
       "min                           0.000000     0.000000  \n",
       "25%                           0.493000    10.100000  \n",
       "50%                           0.677000    12.300000  \n",
       "75%                           0.779000    14.300000  \n",
       "max                           0.948000    20.700000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "life_expectancy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the output of `df.describe()`.\n",
    "\n",
    "**Discussion 01:** Discuss with your lab partner what the `count` row represents. Check  the correct answer on stackoverflow or the official pandas documentation. How is this information helpful when analyzing your dataset? Use stackoverflow or the official documentation to work out how you could compute just one of the statistics for one specific column, e.g. how can we compute the mean of the 'BMI' column? Write down your answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Total expenditure' is not listed in the statistical analysis, which is because, as mentioned above, all values were cast to a string when loaded. This is very annoying, as most values are actually numeric, and the statistical measures of the numeric values, such as the median, are still meaningful, even though not all values are numeric. To compute these values, we will have to clean up the data first. As mentioned above: **Clean data is key!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.B Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B.1 Detecting non-numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common problem in datasets is that data are not consistently of the same type. For example, a date is written in a non-standard way or a number is given as a string instead of a numeric value. Many data analysis algorithms are applied to numeric data. So let's tackle the problem we encountered above in the 'Total expenditure' column.\n",
    "\n",
    "To help us ensure that every entry in a column is numeric, pandas offers the function `pd.to_numeric(series, errors={‘ignore’, ‘raise’, ‘coerce’}, downcast = {‘integer’, ‘signed’, ‘unsigned’, ‘float’})`.\n",
    "\n",
    "This function does three things:\n",
    "1. it checks whether every entry is a numeric value\n",
    "2. if a non-numeric value is encountered (error), either it is ignored, raised (see below) or replaced by a NaN value (`error = 'coerce'`)\n",
    "3. if `downcast` is chosen (default is `None`), it casts all numeric values down to the specified parameter. \n",
    "\n",
    "**Remark:** If the chosen `dtype` occupies more bits  than the `dtype` of the value (e.g. chosen `dtype` is `float`, but the found value is `int`) `pd.to_numeric` doesn't recast the value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       8.16\n",
       "1       8.18\n",
       "2       8.13\n",
       "3       8.52\n",
       "4       7.87\n",
       "        ... \n",
       "2933    7.13\n",
       "2934    6.52\n",
       "2935    6.53\n",
       "2936    6.16\n",
       "2937    7.10\n",
       "Name: Total expenditure, Length: 2938, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "life_expectancy['Total expenditure'] = pd.to_numeric(life_expectancy['Total expenditure'], \n",
    "                                                    errors = 'coerce')\n",
    "life_expectancy['Total expenditure']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark** `pd.numeric` creates a copy and does not apply the changes directly to the series, hence the need to save the changes as shown above. If you just want to check whether or not all values are numeric, you can call `pd.to_numeric(life_expectancy['Total expenditure'], errors='raise')`, which will raise errors but not save any changes.\n",
    "\n",
    "Why is it interesting to have NaN values instead of strings, such as 'No data'? Let's check the type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np.NaN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a float, which means that you can apply all numeric operations to entire columns; NaN values are usually just ignored. Now we can compute the missing values from our statistical analysis above.\n",
    "\n",
    "**Exercise 03:**\n",
    "\n",
    "a) Start by saving the values from the 'Total expenditure' column in a new variable `total_expenditure`. What type does `total_expenditure` have? (Try to think of the answer first, then check with `type()`.)\n",
    "\n",
    "b) Compute the eight values (count, mean, ...) using the methods you should have learned about in **Discussion 01**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** By default, when loading a csv file in pandas, empty values are usually filled with *NaN values automatically*.\n",
    "\n",
    "However, even NaN values can be annoying. Where do you plot a value with x-coordinate 1 and y-coordinate NaN? There are different options for how to deal with NaN values.\n",
    "\n",
    "First, let's start by checking which values are NaN, using `.isna()`, which is a mask (we have discussed masks in lab01). It returns the same data structure as the one it is applied to, with `True` and `False` values instead of the values from the original data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Series:\\n')\n",
    "print(life_expectancy['Life expectancy'].isna())\n",
    "print('\\n\\nDataFrames:\\n')\n",
    "print(life_expectancy.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are no NaN in the 'Life expectancy' column. Let's double check, by summing all the values of each column. (Remember that False == 0, and True == 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy['Life expectancy'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that there are a few missing values, but none of them are at the beginning or the end, so we didn't see them. This is an **important lesson**: using `.head()` gives you a good initial idea of how the data looks like, but it is by no means comprehensive!\n",
    "\n",
    "### 2.B.2 Working with NaN values\n",
    "\n",
    "#### Dropping NaN values\n",
    "\n",
    "Now we know how to find NaN values, let's fix those values. We have a couple of options. First, if you just want to get rid of data entries with NaN values, you can drop them using `.dropna(axis={0 or'index', 1 or'columns}, how={‘any’, ‘all’}, thresh=N, subset=[col1, col2, ...])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy.dropna(axis = 0, how = 'any') # drops rows if any value is a NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we specified `how='any'`, pandas drops a row if *any* column has a NaN value, so  the DataFrame is reduced to 1649 rows from 2938 rows initially. However, we wanted to discuss more ways of handling NaN values, and now we have dropped them. The good news is that  `dropna()` creates a copy, as usual with pandas DataFrame methods, and the original DataFrame is preserved. If you do want to apply those changes, you can use either `df = df.dropna(axis = 0, how = 'any')` or `df.dropna(axis = 0, how = 'any', inplace = True)`. `inplace=True` can be used in many DataFrame methods to apply the operations to the DataFrame.\n",
    "\n",
    "Now that we know that we didn't apply the changes to the DataFrame, let's discuss some more options for dealing with NaNs.\n",
    "\n",
    "Instead of `how = 'any'`, you can choose `'all'`, which drops rows only if *all* values are NaN. You can specify `axis=1`, to look along columns for NaN values and drop columns instead of rows. You can further set `subset=[col1, col2]` to search specific columns or rows for NaN values, and finally you can set a `thresh=N` to specify that at least `N` values need to be **non-NaN** to be kept. Give it a try!\n",
    "\n",
    "**Exercise 03:** Drop columns with more than 20% NaN values. Save the result to a new DataFrame and print that DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have got the original DataFrame without the 'Population' column.\n",
    "\n",
    "#### Overwriting NaN values\n",
    "If you don't want to drop your data entries you can replace the values. You have many options, including: \n",
    "- The mean, max or min value of the column\n",
    "- strings\n",
    "- 0\n",
    "\n",
    "The function needed is `df.fillna(value={scalar, dict}, method={‘bfill’, ‘ffill’}, axis={0, 1}, limit=N)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy.fillna(0).loc[2922] # Get row 2922 of the DataFrame where NaN values are replaced with 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have applied two operations at once to the DataFrame. It shows the row we initially inspected where 'Total expenditure' = 'No data'. Now it is 0.\n",
    "\n",
    "**Remark:** In some cases, it makes more sense to replace different NaN values according to the column they are in. You can do this by passing a dictionary instead of a scalar. `df.fillna(value={'col1': value1, 'col2': value2})`. For more information on `fillna()` you are invited to check the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html).\n",
    "\n",
    "We will only be interested by entries with no NaN values for the rest of this lab, so we will drop all entries with NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy.dropna(subset=['Life expectancy'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.C Unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have loaded the data, we have extracted some high-level information about the data set, and dealt with missing data entries. These steps are important initial steps that need to be taken whenever you deal with data, especially data you haven't generated yourself.\n",
    "\n",
    "Another important step is to check whether your data is free of duplicate observations. Let's check how many unique values we have in each column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, for example, in 'Status', we only have two unique values. Let's check which ones they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(life_expectancy['Status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark:** `df.nunique()` is applied to the data structure (Series or DataFrames), whereas `pd.unique()` takes the data structure as a parameter.\n",
    "\n",
    "The above query showed us that there are two categories in the 'Status' column: Developing and Developed. Let's find out the impact of whether a country is considered to be developing or developed.\n",
    "\n",
    "**Exercise 04:** \n",
    "\n",
    "a) Split the dataset into two subsets with the names 'developed' and 'developing' according to the status of the countries.\n",
    "\n",
    "b) Compute the mean life expectancy of each subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Is the difference of life expectancy between the two groups about what you expected? If not what statistical metric could help you understand why the life expectancy difference is not what you expected? If you can think of a specific metric, compute that metric and see whether this confirms your expectations. Write down your answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have found the unique values, but `pd.unique` searches unique values in each column. To search for entire rows of data that duplicate other rows, we need to use `df.duplicated(subset=[col1, col2, ...], keep={'first', 'last', False})`. This function is a mask; by now you should know how to use masks. It returns a Series with `True` and `False` entries, where `True` marks the rows that are duplicates of another row. The subset parameter allows you to specify which columns to consider when checking for duplicates. This can be very helpful when you have a column of unique identifiers, and you want to know if there are rows that are identical apart from the unique identifier. For example, different patients (with different IDs = unique identifier) might have the same symptoms. The `keep` parameter lets you specify which occurrence of a duplicate to mark as `False` (i.e. not as a duplicate): \n",
    "- `'first'`, which is the default value, marks the first occurrence as not a duplicate, but all other occurrences as duplicates of the first occurrence, \n",
    "- `'last'` marks the last occurrence as not a duplicate, but all other occurrences as duplicates of the last occurrence, and\n",
    "- `False` marks all occurrences of duplicates as duplicates.\n",
    "\n",
    "Let's give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As promised above, `.duplicated()` gives you a mask. It looks like no entry is a duplicate. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Remember, `False==0`, `True==1`; thus summing the series tells you how many `True` entries there were in the Series). The above tells you that there are no duplicates. Thankfully the WHO, as we would hope, created a clean dataset.\n",
    "\n",
    "Let's check whether there are countries in which the life expectancy remained the same over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "life_expectancy.duplicated(subset=['Country', 'Life expectancy']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 136 instances in which the life expectancy did not change. Let's print them.\n",
    "\n",
    "**Exercise 05:** \n",
    "- Print *all* occurrences of duplicates. Hint: think about the keep parameter.\n",
    "- Print all the countries that appear in the list. Each country should only appear once.\n",
    "- Create a new DataFrame from the `life_expectancy` DataFrame, with the name `cleaned_life_expectancy`, in which  each country only appears once. Since the entries are sorted newest to oldest, only keep the newest observation of each country. Print out the first few lines to check your DataFrame looks as expected.\n",
    "\n",
    "**Hint:** to negate a condition in Pandas, use the tilde `~` operator instead of `not`, e.g.: if `people` is a DataFrame with a column `adult` of boolean values: `child = people[~people['adult']]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have finished cleaning our dataset. Let's plot some of the results. You can ignore the code below; we will learn about plotting in the next two labs and this is just to give you an idea of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_life_expectancy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(cleaned_life_expectancy, x_vars = ['Life expectancy'], y_vars = ['percentage expenditure', 'Hepatitis B', 'Measles',\n",
    "       'BMI', 'under-five deaths', 'Polio', 'Total expenditure',\n",
    "       'Diphtheria', 'HIV/AIDS', 'GDP', 'Population',\n",
    "       'thinness 1-19 years', 'thinness 5-9 years',\n",
    "       'Income composition of resources', 'Schooling'], hue='Status', palette='Dark2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Discuss with your lab partner which factors seem to have a clear correlation with life expectancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we have used single datasets and analyzed them. The WHO dataset contains medical and economic factors, and the plot above showed whether or not there was a correlation between those factors and life expectancy. \n",
    "\n",
    "Now suppose that a politician approaches you who is interested in whether limiting alcohol consumption might have a positive effect on life expectancy. To analyze this you need to use a second dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinks_by_country_loc = os.path.join(os.getcwd(), 'datasets', 'drinks_by_country.csv')\n",
    "drinks_by_country = pd.read_csv(drinks_by_country_loc)\n",
    "drinks_by_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.D Combining data from two datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will rarely find all the information you need in one dataset. Being able to combine data from different datasets into one is very important. Practise it until you feel very comfortable; having just tried it once is not enough. \n",
    "\n",
    "There a various ways of combining data from datasets: *concatenation*, *merging* and *joining*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation\n",
    "\n",
    "Let's start with the arguably simplest pandas function: `concat`\n",
    "\n",
    "When you concatenate two *data structures*, pandas DataFrames or pandas series, you simply append one to the other along one axis (along rows or along columns), i.e. you expect that the new data is \"independent\" of the existing data. \n",
    "\n",
    "Let's illustrate that on a simple toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                    'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "                    'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                    'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "                    index=[0, 1, 2, 3])\n",
    "\n",
    "df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n",
    "                    'B': ['B4', 'B5', 'B6', 'B7'],\n",
    "                    'C': ['C4', 'C5', 'C6', 'C7'],\n",
    "                    'D': ['D4', 'D5', 'D6', 'D7']},\n",
    "                    index=[0, 1, 2, 3])\n",
    "\n",
    "print(df1)\n",
    "print('\\n\\n')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1, df2])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:** \n",
    "- Both DataFrames had entries with indices 0-3. However, using `concat` means that there is no relational connection between the rows, and thus `df2` is just appended to `df1`.\n",
    "- `pd.concat()` takes as input a list of data structures and can concatenate several DataFrames at once.\n",
    "\n",
    "As mentioned above, we can specify along which axis the data structures should be concatenated. The default is `axis=0`, as above. We hinted above that `concat` and `join` have two different meanings. In fact, `join={'inner', 'outer'}` is a parameter allowing you to specify the set logic of how data entries that only exist in one set but not the other should be treated. Using *inner* join means that you only want the intersection, while *outer* join means you want to keep the union. Sounds a bit abstract? Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                    'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "                    'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "                    index=[0, 1, 2, 3])\n",
    "\n",
    "df4 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n",
    "                    'B': ['B4', 'B5', 'B6', 'B7'],\n",
    "                    'C': ['C4', 'C5', 'C6', 'C7']},\n",
    "                    index=[0, 1, 2, 3])\n",
    "\n",
    "print(df3)\n",
    "print('\\n\\n')\n",
    "print(df4)\n",
    "\n",
    "result = pd.concat([df3, df4], axis=0, join='outer')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above along `axis=0`, one dataset has the columns 'A', 'B', and 'D', while the other has the columns 'A', 'B', and 'C'. Because we specified `join='outer'` it took the *union* of the two column sets, i.e. 'A', 'B', 'C', and 'D' and just filled out the missing information with `NaN` values. What would happen in the case of `join='inner'`? Give it a try. Is it what you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have seen, that indices 0-3 were repeated, because we just appended one dataset to the other and didn't assume any relation. However, in that case the indices most probably don't even have a meaning, and could be ignored. This can be achieved with, who could have guessed, `ignore_index=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df1, df2], ignore_index=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging\n",
    "\n",
    "What can we do, when we have a relation between two datasets? For example, in both the life expectancy dataset and the alcohol consumption dataset, we have the column 'Country'. Combining both datasets by comparing those columns and combining the corresponding data entries would be very helpful.\n",
    "\n",
    "Let's consider a toy example again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = pd.DataFrame({'key': ['K0', 'K1', 'K2'],\n",
    "                     'A': ['A0', 'A1', 'A2'],\n",
    "                     'B': ['B0', 'B1', 'B2']})\n",
    "\n",
    "right = pd.DataFrame({'key': ['K1', 'K0', 'K3'],\n",
    "                      'C': ['C0', 'C1', 'C3'],\n",
    "                      'D': ['D0', 'D1', 'D3']})\n",
    "print(left)\n",
    "print('\\n\\n')\n",
    "print(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, on='key')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `on='column'` we specify the column the two datasets will be compared on.\n",
    "\n",
    "**Remarks**\n",
    "- In contrast to `pd.concat()`, `pd.merge()` only takes two DataFrames as input (left and right)\n",
    "- `pd.merge()` compares the rows across all data entries. For example, in the 'right' dataset the data entry 'K0' is in the second row though in the left data set it is in the first row.\n",
    "- As you can see above, by default, `pd.merge()` uses an inner join.\n",
    "- The column specified by the keyword 'on' needs to be *written the same way* in both datasets.\n",
    "\n",
    "With `pd.concat()` the 'join' parameter specifies an inner join and an outer join. With `pd.merge()` we have the same option, only the keyword is `how={'inner', 'outer', 'left', 'right'}`, with the two extra options `left` and `right`. Technically `left` stands for 'left outer join', and means that:\n",
    "- we keep all rows that appear in the left dataset\n",
    "- merge the matching rows from the right dataset into the left dataset\n",
    "- drop all rows that only appear in the right dataset.\n",
    "\n",
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(left, right, on='key', how='left')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'K0', 'K1', and 'K2' are kept, as they all appear in the `left` dataset. However, 'K3', which only appears in the right dataset is dropped.\n",
    "\n",
    "**Note on performance**: By default, `merge` will sort the output based on the values in the key column in lexicographic order, which can affect the performance substantially, if you do not care about the order, you can set `sort=False`.\n",
    "\n",
    "There are a couple of more parameters you can set, in both `concat` and `merge`. However, the above are the most important. If you are interested, check out the [pandas website](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html).\n",
    "\n",
    "**Exercise 06:** Use either `pd.merge` or `pd.concat` to combine the two datasets(`cleaned_life_expectancy`, `drinks_by_country`) so we can relate drinks consumption to life expectancy. We don't want to have unnecessary NaN values, so only keep the entries that are found in both datasets. Save the result to `final_life_expectancy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, ignore the code below. Just run it and compare the impact of different drinks on life expectancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(final_life_expectancy, x_vars = ['Life expectancy'], y_vars = ['Beer Servings', 'Spirit Servings', 'Wine Servings', 'Total Litres of Pure Alcohol'], hue='Status', palette='Dark2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** \n",
    "- Does the data above suggest that alcohol consumption as an average for a population has an impact on life expectancy?\n",
    "- Find the three countries with the lowest life expectancy from `final_life_expectancy`.\n",
    "- Have a look at the plots above, and discuss with your lab partner: Can we pinpoint the short life expectancy to specific factors? What changes would help those countries probably most to improve their life expectancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer: \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.4 ('fdslabsenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "8b0af48e3506369dc2e8bd191aaab652da2d9dd7f3b323d15d519516f732b4cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
